<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ACGD: Visual Multitask Policy Learning with Asymmetric Critic Guided Distillation</title>
    <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100">
    <main>
        <!-- Existing header section -->
        <section class="relative items-center w-full px-5 pt-12 pb-2 mx-auto md:px-12 lg:px-16 max-w-7xl lg:pt-24 lg:pb-4">
            <!-- ... (keep existing content) ... -->
        </section>

        <!-- Existing abstract section -->
        <section class="flex flex-col items-center px-5 pt-8 pb-6 my-6 mx-auto max-w-5xl sm:px-6 lg:px-8">
            <!-- ... (keep existing content) ... -->
        </section>

        <!-- New section for ACGD -->
        <section class="flex flex-col items-center px-5 pt-9 pb-3 mx-auto max-w-6xl sm:px-6 lg:px-8">
            <div class="flex flex-col w-full max-w-6xl mx-auto prose text-left text-gray-800">
                <h2 class="text-2xl font-bold pb-5"><span class="grad_text">Asymmetric Critic-Guided Distillation (ACGD)</span></h2>
                <p>ACGD is a framework for learning multi-task dexterous manipulation policies that can manipulate articulated objects using images as input. It uses a student-teacher distillation approach to distill multiple expert policies into a single vision-based, multi-task student policy for dexterous manipulation.</p>
                
                <h3 class="text-xl font-bold mt-6">Key Components</h3>
                <ul>
                    <li>Expert policies trained with RL using privileged state information</li>
                    <li>Student policy that takes camera images and robot proprioception as input</li>
                    <li>Critic-informed distillation loss function</li>
                    <li>Vector-quantized variational autoencoder (VQ-VAE) policy architecture</li>
                </ul>

                <h3 class="text-xl font-bold mt-6">Method Overview</h3>
                <p>ACGD consists of two main stages:</p>
                <ol>
                    <li>Train single-task expert policies and critics using privileged state information</li>
                    <li>Distill a multi-task student policy using expert rollouts and critics</li>
                </ol>
                <p>The student policy uses a VQ-VAE architecture with a transformer encoder and decoder to predict discrete action tokens from image observations and robot state.</p>

                <h3 class="text-xl font-bold mt-6">Experimental Results</h3>
                <p>ACGD was evaluated on three multi-task domains:</p>
                <ul>
                    <li>MyoDex: 10 tasks using a single hand with 39 DoF muscle tendons</li>
                    <li>BiDex: 3 tasks using two Shadow hands with 52 DoF total</li>
                    <li>OpDex: 2 tasks using an AllegroHand with 22 DoF and operable articulated objects</li>
                </ul>
                <p>ACGD outperforms baselines like BC-RNN+DAgger, ACT, and MT-PPO on various dexterous manipulation benchmarks, achieving 10-15% improvement over baseline algorithms.</p>

                <h3 class="text-xl font-bold mt-6">Key Advantages</h3>
                <ul>
                    <li>Efficient offline distillation without requiring additional environment interactions</li>
                    <li>Scales well with number of tasks and demonstrations</li>
                    <li>Performs well on high-DoF multi-task visuomotor skills</li>
                </ul>

                <h3 class="text-xl font-bold mt-6">Limitations</h3>
                <ul>
                    <li>Performance can be limited by suboptimal expert critics</li>
                    <li>Relies on goal images from expert trajectories for conditioning</li>
                    <li>Current implementation only uses success-only trajectories for training</li>
                </ul>

                <h3 class="text-xl font-bold mt-6">Videos</h3>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mt-4">
                    <video src="videos/acgd_overview.mp4" controls>ACGD Overview</video>
                    <video src="videos/myodex_tasks.mp4" controls>MyoDex Task Set</video>
                    <video src="videos/bidex_tasks.mp4" controls>BiDex Task Set</video>
                    <video src="videos/opdex_tasks.mp4" controls>OpDex Task Set</video>
                    <video src="videos/performance_comparison.mp4" controls>Performance Comparison</video>
                </div>
            </div>
        </section>

        <!-- Existing sections -->
        <!-- ... (keep existing content for Motivation, Method Overview, Real Robot Experiments, and Simulation Experiments) ... -->

        <!-- Existing BibTeX section -->
        <section class="flex flex-col items-center px-5 pt-0 pb-12 mx-auto max-w-6xl sm:px-6 lg:px-8">
            <!-- ... (keep existing content) ... -->
        </section>
    </main>

    <footer class="px-4 py-8 mx-auto bg-gray-50 w-full sm:px-6 lg:px-16">
        <div class="flex flex-wrap items-baseline lg:justify-center">
            <p class="text-sm text-center font-light text-gray-600">
                If you have any questions, please contact Krishnan Srinivasan (krshna "at" stanford "dot" edu).
            </p>
        </div>
    </footer>
</body>
</html>
